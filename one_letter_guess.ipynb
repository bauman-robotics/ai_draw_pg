{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33ee003-3020-4791-b7b0-fc2272958101",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add to HEAD ======\n",
    "# def main():\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "#import ai_funcs\n",
    "\n",
    "\n",
    "# from ai_funcs import unzip_data\n",
    "# from ai_funcs import check_cuda\n",
    "from ai_funcs import dataset_transform\n",
    "# from ai_funcs import dataset_divide\n",
    "# from ai_funcs import plot_image_from_datset_by_name\n",
    "# from ai_funcs import plot_image_from_datset_by_index\n",
    "# from ai_funcs import tensor_to_image_unnormalize\n",
    "# from ai_funcs import get_label_from_datset_by_index\n",
    "# from ai_funcs import get_image_from_datset_by_index\n",
    "# from ai_funcs import get_letter_by_number\n",
    "# from ai_funcs import get_letter_by_index\n",
    "# from ai_funcs import imshow\n",
    "# from ai_funcs import imshow_save\n",
    "# from ai_funcs import tensor_show\n",
    "# from ai_funcs import get_class_name\n",
    "# from ai_funcs import plot_count_of_images_from_datset_by_index\n",
    "# from ai_funcs import test_model_by_dataset\n",
    "\n",
    "# from ai_funcs import finetune_model\n",
    "from ai_funcs import train_model\n",
    "from ai_funcs import predict_image\n",
    "from ai_funcs import image_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0200c5a-2d63-422a-b3dc-ce04bdbc2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for jupyter ----> path = os.getcwd()\n",
    "# path = os.getcwd()\n",
    "#for python ----> path = os.path.dirname(os.path.abspath(__file__))\n",
    "path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Определяем имена файлов\n",
    "# model_name = path + 'model_self.pth'\n",
    "# checkpoint_name = path + 'checkpoint_self.pth'\n",
    "# train_dataset_dir = path + 'Upload'\n",
    "model_name      = os.path.join(path, 'model_self.pth')\n",
    "checkpoint_name = os.path.join(path, 'checkpoint_self.pth')\n",
    "train_dataset_dir = os.path.join(path, 'Upload')\n",
    "\n",
    "# Путь к тестовому изображению\n",
    "# test_image_path = path + 'Test_Img/'\n",
    "test_image_name = 'Test.png'\n",
    "test_image_path = os.path.join(path, 'Test_Img/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "330a105a-8eec-4940-9bf1-8bd719024cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset)= 9\n",
      "len(dataset) 9\n",
      "num_classes = 2\n"
     ]
    }
   ],
   "source": [
    "# #============================= CNN ======================================================\n",
    "#Аугментация данных:\n",
    "#======================================\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(224),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = dataset_transform(train_dataset_dir, transform)\n",
    "\n",
    "# # Разделяем данные на тренировочную и контрольную выборки\n",
    "# train_dataset, val_dataset = dataset_divide(dataset)\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(root='path_to_train_data', transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "#======================================\n",
    "\n",
    "print('len(dataset)', len(dataset))\n",
    "# print('len(train_dataset)', len(train_dataset))\n",
    "# print('len(val_dataset)', len(val_dataset))\n",
    "num_classes = len(dataset.classes)\n",
    "print('num_classes =', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "395dea8b-09a9-4494-b243-d2d61a15d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================ nn =========================================\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7, 512)  # Adjusted for additional pooling layers\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = x.view(-1, 512 * 7 * 7)  # Adjusted for additional pooling layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b86dd3d0-84af-4ed9-9008-7ee3a60a7787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path_name = /home/andrey/projects/ai/draw/model_self.pth\n",
      "checkpoint_path_name = /home/andrey/projects/ai/draw/checkpoint_self.pth\n",
      "num_classes = 2\n"
     ]
    }
   ],
   "source": [
    "#============================= CNN ======================================================\n",
    "# Получаем текущий рабочий каталог\n",
    "# path = os.getcwd()\n",
    "\n",
    "# # Определяем имена файлов\n",
    "# model_name = 'model.pth'\n",
    "# checkpoint_name = 'checkpoint.pth'\n",
    "\n",
    "# Создаем полные пути к файлам\n",
    "model_path_name = os.path.join(path, model_name)\n",
    "checkpoint_path_name = os.path.join(path, checkpoint_name)\n",
    "\n",
    "# Выводим результаты\n",
    "print('model_path_name =', model_path_name)\n",
    "print('checkpoint_path_name =', checkpoint_path_name)\n",
    "\n",
    "# Параметры модели\n",
    "num_classes = len(dataset.classes)\n",
    "print('num_classes =', num_classes)\n",
    "model = CNNClassifier(num_classes)\n",
    "trained_model = CNNClassifier(num_classes)\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "if os.path.exists(model_path_name):\n",
    "    #print(trained_model)\n",
    "    \n",
    "    checkpoint = torch.load(model_path_name)\n",
    "    trained_model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    # Обучение модели\n",
    "    trained_model, losses, loss_history = train_model(model, criterion, optimizer, scheduler, train_loader, num_epochs=30)\n",
    "    \n",
    "    # Сохранение модели и чекпоинтов\n",
    "    torch.save({\n",
    "        'model_state_dict': trained_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'losses': losses,\n",
    "        'loss_history': loss_history,\n",
    "    }, checkpoint_path_name)\n",
    "    \n",
    "    torch.save(trained_model.state_dict(), model_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f191ae5-13bf-4ab3-b2b2-b867ee9ab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============  fine tune CNN ========================================================================= \n",
    "# data_dir = 'my_test_dataset'\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     # transforms.RandomResizedCrop(224),\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     # transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# # losses = []\n",
    "# # loss_history = []\n",
    "\n",
    "# # Дообучение модели на новом датасете\n",
    "# trained_model, losses, loss_history = finetune_model(trained_model, train_loader, criterion, optimizer, scheduler, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e96dba-c0d6-4456-ade3-aa9336935e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "trained_model.eval()\n",
    "\n",
    "# Получаем предсказание\n",
    "print(f'test_image_path: {test_image_path}')\n",
    "print(f'test_image_name: {test_image_name}')\n",
    "predicted_class = predict_image(test_image_path, test_image_name, trained_model, transform)\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "\n",
    "#    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456a871-5f2f-4264-bda3-584c0b3f19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD to end ########\n",
    "#     return predicted_class\n",
    "#     # !jupyter nbconvert --to script letters.ipynb\n",
    "#     pass\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()  # скрипт запускается непосредственно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d12973a0-4169-4b93-8c0d-ac6532f6440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook one_letter_guess.ipynb to script\n",
      "[NbConvertApp] Writing 7399 bytes to one_letter_guess.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script one_letter_guess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ee490-3433-4769-b3e9-c09b08a00042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
